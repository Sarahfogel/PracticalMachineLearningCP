{
    "contents" : "#==============================================================================\n#\n# Exploratory Analysis for Practical Machine Learning Course Project\n#\n#==============================================================================\n\n\n#=======================Download and Read in Files=============================\n\n# First Download the files, if they haven't already been downloaded\n\n    if (\"pml-training.csv\" %in% list.files(\".\")) {\n        \n    } else { print(\"downloading\")\n        download.file(\n            \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\",\n            destfile=\"./pml-training.csv\")\n    }\n\n    if (\"pml-testing.csv\" %in% list.files(\".\")) {\n        \n    } else { print(\"downloading\")\n             download.file(\n                 \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\",\n                 destfile=\"./pml-testing.csv\")\n    }\n\n# Now read the files in\n\n    training<-read.csv(\"./pml-training.csv\", header=T)\n    testing<-read.csv(\"./pml-testing.csv\", header=T)\n\n\n#==========================Explore Data=========================================\n\n# There are 160 variables and nearly 20000 observations in this dataset\n\n    summary(training[,1:20])\n    summary(training[,21:40])\n    summary(training[,41:60])\n    summary(training[,61:80])\n    summary(training[,81:100])\n    summary(training[,101:120])\n    summary(training[,121:140])\n    summary(training[,141:160])\n\n# List of variables that should be removed: X - just an index, time stamps (x3) - time\n#    shouldn't matter, new_window and num-window - what is that?,\n# Anything that has 19216 NAs or more - these only have a 2% chance of having data:\n#   columns 12:36, 50:59, 69:83, 87:101, 103:112, 125:139, 141:150\n\n\n    dim(training[-c(1,3:7, 12:36, 50:59, 69:83, 87:101, 103:112,125:139, 141:150  )])\n\n# Create new training and testing sets with the irrelevant variables removed and partition off a\n#   validation set\n    library(caret)\n    set.seed(3450)\n\n    trainIndex<-createDataPartition(training[,160], p=.8, list=FALSE)\n\n    validationclean<-training[-trainIndex,-c(1,2,3:7, 12:36, 50:59, 69:83, 87:101, 103:112,125:139, 141:150  )]\n    trainingclean<-training[trainIndex, -c(1,2,3:7, 12:36, 50:59, 69:83, 87:101, 103:112,125:139, 141:150  )]\n    testingclean<-testing[-c(1,2,3:7, 12:36, 50:59, 69:83, 87:101, 103:112,125:139, 141:150  )]\n\n# Also, create a vector containig the actual outcome we're interested in for this analysis\n#   correct or not, not which class of incorrect\n\n    outcomes<-factor(levels=c(\"Right\", \"Wrong\"))\n    for (i in 1:dim(trainingclean)[1]) {\n        if (trainingclean[i,53]==\"A\"){\n            outcomes[i]<-\"Right\"\n        } else {\n            outcomes[i]<-\"Wrong\"\n        }\n    }\n\n    validationoutcomes<-factor(levels=c(\"Right\", \"Wrong\"))\n    for (i in 1:dim(validationclean)[1]) {\n        if (validationclean[i,53]==\"A\"){\n            validationoutcomes[i]<-\"Right\"\n        } else {\n            validationoutcomes[i]<-\"Wrong\"\n        }\n    }\n# Continue exploring\n    summary(trainingclean)\n\n# The 53 variables we have left are; roll, pitch, and yaw of each of 4 sensors(12); \n#   total acceleration of 4 sensors(4); acceleration in 3 directions of 4 sensors (12);\n#   gyroscopic forces in 3 directions on 4 sensors (12); \n#   magnetic force in 3 directions on 4 sensors (12); and class of exercise (1)\n\n# Check if total acceleration adds anything over the 3 directions\n    d<-(sqrt(accel_belt_x^2+accel_belt_y^2+accel_belt_x^2)-total_accel_belt)\n    head(d)\n\n    d2<-(sqrt(roll_belt^2+pitch_belt^2+yaw_belt^2)-total_accel_belt)\n    head(d2)\n\n    d3<-(sqrt(gyros_belt_x^2+gyros_belt_y^2+gyros_belt_x^2)-total_accel_belt)\n    head(d3)\n\n    head(total_accel_belt)\n\n# Look for variables with near 0 variability\n\n    nsv<- nearZeroVar(trainingclean, saveMetrics=TRUE)\n    nsv\n\n    #No variables come out small variation\n\n# Look for highly correlated variables\n    M<-abs(cor(trainingclean[,1:52]))\n    diag(M)<-0\n    which(M>.8, arr.ind=T)\n    # Lots of high correlations\n\n# Quick graphs of all remaining predictors to look for skewness etc.\n    par(mfrow=c(2,2))\n    for (i in 1:4) {\n        hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 5:8) {\n        hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 9:12) {\n        hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 13:16) {\n        hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 17:20) {\n        hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 21:24) {\n        hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 25:28) {\n        hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 29:32) {\n        hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 33:36) {\n        hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 37:40) {\n        hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 41:44) {\n        hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 45:48) {\n        hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 49:52) {\n        hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n    }\n# Boxplots\n    for (i in 1:4) {\n        boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 5:8) {\n        boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 9:12) {\n        boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 13:16) {\n        boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 17:20) {\n        boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 21:24) {\n        boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 25:28) {\n        boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 29:32) {\n        boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 33:36) {\n        boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 37:40) {\n        boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 41:44) {\n        boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 45:48) {\n        boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n    }\n    for (i in 49:52) {\n        boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n    }\n    par(mfrow=c(1,1))\n   \n\n# Consider pca - must apply same pcas to test set as used for training set!\n# Sample code:\n\n    preProc<-preProcess(log10(trainingclean[,1:52]+.0001), method=\"pca\", pcaComp=2)\n    pcatrain<- predict(preProc, log10(trainingclean[,1:52]+.0001))\n    modelFit<- train(outcomes~., data=pcatrain, method=\"glm\")\n\n    pcatest<- predict(preProc, log10(testingclean[,1:52] +.0001))\n    confusionMatrix(outcomes, predict(modelFit, testPC))\n\npreProc<-preProcess(trainingclean[,1:52], method=\"pca\", thresh=.9)\npcatrain<- predict(preProc, trainingclean[,1:52])\nmodelFit<- train(outcomes~., data=pcatrain, method=\"glm\")\n\nmodelFit$result\n\npcavalidation<- predict(preProc,validationclean[,1:52])\nconfusionMatrix(validationoutcomes, predict(modelFit, pcavalidation))\n\n\n# Try out a few models at this point\n    library(caret)\n    glm.model<-train(trainingclean[,1:52], outcomes, method=\"glm\", na.action=na.omit)\n\n    glm.model$results     #Accuracy of .903, not bad, but measuring the wrong thing\n\n    confusionMatrix(outcomes, predict(glm.model, trainingclean[,1:52]))\n    confusionMatrix(validationoutcomes, predict(glm.model, validationclean[,1:52]))\n\n    rpart.model<-train(trainingclean[,1:52], trainingclean[,53], method=\"rpart\") #began at 10:28, done by 10:38\n    rpart.model$result\n    confusionMatrix(validationclean[,53], predict(rpart.model, validationclean[,1:52]))\n        #Accuracy of .4876, not very good\n\n    rpart.model\n    rpart.model$finalModel\n    plot(rpart.model$finalModel, uniform=T, main=\"Classification Tree\")\n    text(rpart.model$finalModel, use.n=T, all=T, cex=.5)\n\n    rf.model<-train(trainingclean[,1:52], trainingclean[,53], method=\"rf\") #Started this at 8:45 today, done by 9:49, possibly earlier\n    rf.model$result\n    confusionMatrix(validationclean[,53], predict(rf.model, validationclean[,1:52]))\n        #Accuracy of .9939 on validation set, excellent\n\n    rf.model\n    rf.model$finalModel\n    plot(rf.model$finalModel)\n\n#Try out some 10 fold cross validation on this model\n    fitControl<-trainControl(method=\"repeatedcv\", number=10, repeats=10)\n    rf.model.cv<-train(trainingclean[,1:52], trainingclean[,53], method=\"rf\", trControl=fitControl) #began 10:53, not done by 12:02\n    system.time(rf.model.cv<-train(trainingclean[,1:52], trainingclean[,53], method=\"rf\", trControl=fitControl)) \n    rf.model.cv\n    trellis.par.set(caretTheme())\n    plot(rf.model.cv)\n    trellis.par.set()\n    confusionMatrix(validationclean[,53], predict(rf.model.cv, validationclean[,1:52]))\n\n\n\n    rf.pca.model<-train(pcatrain, trainingclean[,53], method=\"rf\") #began at 9:52, done by 10:27\n    rf.pca.model$result\n    confusionMatrix(validationclean[,53], predict(rf.pca.model, pcavalidation))\n        #Accuracy of .9748 on validation set, great but less than rf.model\n\n# What other models should I run?\n\n# Naive Bayes\n    nb.model<-train(trainingclean[,1:52], trainingclean[,53], method=\"nb\") \n    nb.model$result\n    confusionMatrix(validationclean[,53], predict(nb.model, validationclean[,1:52]))\n        #Accuracy .7487, not as good as rf.model\n# Neural net deep learning?\n\n    set.seed(4056)\n    smalltraining<-trainingclean[sample(1:dim(trainingclean)[1], size=5000), ]\n    nnet.model.small<-train(smalltraining[,1:52], smalltraining[,53], method=\"nnet\") \n    nnet.model.small\n    confusionMatrix(validationclean[,53], predict(nnet.model.small, validationclean[,1:52]))\n        #Accuracy .3849 with 5000 observations\n\n    gbm.model<-train(smalltraining[,1:52], smalltraining[,53], method=\"gbm\") \n    gbm.model\n    confusionMatrix(validationclean[,53], predict(gbm.model, validationclean[,1:52]))\n        #Accuracy of .9467 with only 5000 variables - try this one full size\n\n",
    "created" : 1431989057855.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2233521078",
    "id" : "CAACB702",
    "lastKnownWriteTime" : 1432068166,
    "path" : "P:/Education/Coursera Courses/Practical Machine Learning/PracticalMachineLearningCP/Exploratory data analysis.R",
    "project_path" : "Exploratory data analysis.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_source"
}