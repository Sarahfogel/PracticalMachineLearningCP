{
    "contents" : "#==============================================================================\n#\n# Exploratory Analysis for Practical Machine Learning Course Project\n#\n#==============================================================================\n\n\n#=======================Download and Read in Files=============================\n\n# First Download the files, if they haven't already been downloaded\n\nif (\"pml-training.csv\" %in% list.files(\".\")) {\n    \n} else { print(\"downloading\")\n         download.file(\n             \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\",\n             destfile=\"./pml-training.csv\")\n}\n\nif (\"pml-testing.csv\" %in% list.files(\".\")) {\n    \n} else { print(\"downloading\")\n         download.file(\n             \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\",\n             destfile=\"./pml-testing.csv\")\n}\n\n# Now read the files in\n\ntraining<-read.csv(\"./pml-training.csv\", header=T)\ntesting<-read.csv(\"./pml-testing.csv\", header=T)\n\n\n#==========================Explore Data=========================================\n\n# There are 160 variables and nearly 20000 observations in this dataset\n\nsummary(training[,1:20])\nsummary(training[,21:40])\nsummary(training[,41:60])\nsummary(training[,61:80])\nsummary(training[,81:100])\nsummary(training[,101:120])\nsummary(training[,121:140])\nsummary(training[,141:160])\n\n# List of variables that should be removed: X - just an index, time stamps (x3) - time\n#    shouldn't matter, new_window and num-window - what is that?,\n# Anything that has 19216 NAs or more - these only have a 2% chance of having data:\n#   columns 12:36, 50:59, 69:83, 87:101, 103:112, 125:139, 141:150\n\n\ndim(training[-c(1,3:7, 12:36, 50:59, 69:83, 87:101, 103:112,125:139, 141:150  )])\n\n# Create new training and testing sets with the irrelevant variables removed and partition off a\n#   validation set\nlibrary(caret)\nset.seed(3450)\n\ntrainIndex<-createDataPartition(training[,160], p=.8, list=FALSE)\n\nvalidationclean<-training[-trainIndex,-c(1,2,3:7, 12:36, 50:59, 69:83, 87:101, 103:112,125:139, 141:150  )]\ntrainingclean<-training[trainIndex, -c(1,2,3:7, 12:36, 50:59, 69:83, 87:101, 103:112,125:139, 141:150  )]\ntestingclean<-testing[-c(1,2,3:7, 12:36, 50:59, 69:83, 87:101, 103:112,125:139, 141:150  )]\n\n# Also, create a vector containig the actual outcome we're interested in for this analysis\n#   correct or not, not which class of incorrect\n\noutcomes<-factor(levels=c(\"Right\", \"Wrong\"))\nfor (i in 1:dim(trainingclean)[1]) {\n    if (trainingclean[i,53]==\"A\"){\n        outcomes[i]<-\"Right\"\n    } else {\n        outcomes[i]<-\"Wrong\"\n    }\n}\n\nvalidationoutcomes<-factor(levels=c(\"Right\", \"Wrong\"))\nfor (i in 1:dim(validationclean)[1]) {\n    if (validationclean[i,53]==\"A\"){\n        validationoutcomes[i]<-\"Right\"\n    } else {\n        validationoutcomes[i]<-\"Wrong\"\n    }\n}\n# Continue exploring\nsummary(trainingclean)\n\n# The 53 variables we have left are; roll, pitch, and yaw of each of 4 sensors(12); \n#   total acceleration of 4 sensors(4); acceleration in 3 directions of 4 sensors (12);\n#   gyroscopic forces in 3 directions on 4 sensors (12); \n#   magnetic force in 3 directions on 4 sensors (12); and class of exercise (1)\n\n# Check if total acceleration adds anything over the 3 directions\nd<-(sqrt(accel_belt_x^2+accel_belt_y^2+accel_belt_x^2)-total_accel_belt)\nhead(d)\n\nd2<-(sqrt(roll_belt^2+pitch_belt^2+yaw_belt^2)-total_accel_belt)\nhead(d2)\n\nd3<-(sqrt(gyros_belt_x^2+gyros_belt_y^2+gyros_belt_x^2)-total_accel_belt)\nhead(d3)\n\nhead(total_accel_belt)\n\n# Look for variables with near 0 variability\n\nnsv<- nearZeroVar(trainingclean, saveMetrics=TRUE)\nnsv\n\n#No variables come out small variation\n\n# Look for highly correlated variables\nM<-abs(cor(trainingclean[,1:52]))\ndiag(M)<-0\nwhich(M>.8, arr.ind=T)\n# Lots of high correlations\n\n# Quick graphs of all remaining predictors to look for skewness etc.\npar(mfrow=c(2,2))\nfor (i in 1:4) {\n    hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n}\nfor (i in 5:8) {\n    hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n}\nfor (i in 9:12) {\n    hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n}\nfor (i in 13:16) {\n    hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n}\nfor (i in 17:20) {\n    hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n}\nfor (i in 21:24) {\n    hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n}\nfor (i in 25:28) {\n    hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n}\nfor (i in 29:32) {\n    hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n}\nfor (i in 33:36) {\n    hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n}\nfor (i in 37:40) {\n    hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n}\nfor (i in 41:44) {\n    hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n}\nfor (i in 45:48) {\n    hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n}\nfor (i in 49:52) {\n    hist(trainingclean[,i], main=paste(names(trainingclean)[i]))\n}\n# Boxplots\nfor (i in 1:4) {\n    boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n}\nfor (i in 5:8) {\n    boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n}\nfor (i in 9:12) {\n    boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n}\nfor (i in 13:16) {\n    boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n}\nfor (i in 17:20) {\n    boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n}\nfor (i in 21:24) {\n    boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n}\nfor (i in 25:28) {\n    boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n}\nfor (i in 29:32) {\n    boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n}\nfor (i in 33:36) {\n    boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n}\nfor (i in 37:40) {\n    boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n}\nfor (i in 41:44) {\n    boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n}\nfor (i in 45:48) {\n    boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n}\nfor (i in 49:52) {\n    boxplot(trainingclean[,i]~trainingclean[,53], main=paste(names(trainingclean)[i]))\n}\npar(mfrow=c(1,1))\n\n\n# Consider pca - must apply same pcas to test set as used for training set!\n# Sample code:\n\npreProc<-preProcess(log10(trainingclean[,1:52]+.0001), method=\"pca\", pcaComp=2)\npcatrain<- predict(preProc, log10(trainingclean[,1:52]+.0001))\nmodelFit<- train(outcomes~., data=pcatrain, method=\"glm\")\n\npcatest<- predict(preProc, log10(testingclean[,1:52] +.0001))\nconfusionMatrix(outcomes, predict(modelFit, testPC))\n\npreProc<-preProcess(trainingclean[,1:52], method=\"pca\", thresh=.9)\npcatrain<- predict(preProc, trainingclean[,1:52])\nmodelFit<- train(outcomes~., data=pcatrain, method=\"glm\")\n\nmodelFit$result\n\npcavalidation<- predict(preProc,validationclean[,1:52])\nconfusionMatrix(validationoutcomes, predict(modelFit, pcavalidation))\n\n\n# Try out a few models at this point\nlibrary(caret)\nglm.model<-train(trainingclean[,1:52], outcomes, method=\"glm\", na.action=na.omit)\n\nglm.model$results     #Accuracy of .903, not bad, but measuring the wrong thing\n\nconfusionMatrix(outcomes, predict(glm.model, trainingclean[,1:52]))\nconfusionMatrix(validationoutcomes, predict(glm.model, validationclean[,1:52]))\n\nrpart.model<-train(trainingclean[,1:52], trainingclean[,53], method=\"rpart\") #began at 10:28, done by 10:38\nrpart.model$result\nconfusionMatrix(validationclean[,53], predict(rpart.model, validationclean[,1:52]))\n#Accuracy of .4876, not very good\n\nrpart.model\nrpart.model$finalModel\nplot(rpart.model$finalModel, uniform=T, main=\"Classification Tree\")\ntext(rpart.model$finalModel, use.n=T, all=T, cex=.5)\n\nrf.model<-train(trainingclean[,1:52], trainingclean[,53], method=\"rf\") #Started this at 8:45 today, done by 9:49, possibly earlier\nrf.model$result\nconfusionMatrix(validationclean[,53], predict(rf.model, validationclean[,1:52]))\n#Accuracy of .9939 on validation set, excellent\n\nrf.model\nrf.model$finalModel\nplot(rf.model$finalModel)\n\n#Try out some 10 fold cross validation on this model\nfitControl<-trainControl(method=\"repeatedcv\", number=10, repeats=5)\nrf.model.cv<-train(trainingclean[,1:52], trainingclean[,53], method=\"rf\", trControl=fitControl) #\nsystem.time(rf.model.cv<-train(trainingclean[,1:52], trainingclean[,53], method=\"rf\", trControl=fitControl)) \nrf.model.cv\ntrellis.par.set(caretTheme())\nplot(rf.model.cv)\ntrellis.par.set()\nconfusionMatrix(validationclean[,53], predict(rf.model.cv, validationclean[,1:52]))\n\n\n\nrf.pca.model<-train(pcatrain, trainingclean[,53], method=\"rf\") #began at 9:52, done by 10:27\nrf.pca.model$result\nconfusionMatrix(validationclean[,53], predict(rf.pca.model, pcavalidation))\n#Accuracy of .9748 on validation set, great but less than rf.model\n\n# What other models should I run?\n\n# Naive Bayes\nnb.model<-train(trainingclean[,1:52], trainingclean[,53], method=\"nb\") \nnb.model$result\nconfusionMatrix(validationclean[,53], predict(nb.model, validationclean[,1:52]))\n#Accuracy .7487, not as good as rf.model\n# Neural net deep learning?\n\nset.seed(4056)\nsmalltraining<-trainingclean[sample(1:dim(trainingclean)[1], size=5000), ]\nnnet.model.small<-train(smalltraining[,1:52], smalltraining[,53], method=\"nnet\") \nnnet.model.small\nconfusionMatrix(validationclean[,53], predict(nnet.model.small, validationclean[,1:52]))\n#Accuracy .3849 with 5000 observations\n\ngbm.model.small<-train(smalltraining[,1:52], smalltraining[,53], method=\"gbm\") \ngbm.model.small\nconfusionMatrix(validationclean[,53], predict(gbm.model.small, validationclean[,1:52]))\n#Accuracy of .9467 with only 5000 variables - try this one full size\n\nset.seed(333)\ngbm.model<-train(trainingclean[,1:52], trainingclean[,53], method=\"gbm\") #began 9:26 done by 10:40\ngbm.model\nconfusionMatrix(validationclean[,53], predict(gbm.model, validationclean[,1:52]))\n#Accuracy .9595, less than rf.model\n\n# Create answer files for submission\nanswers = predict(rf.model, testingclean)\n#    answers\n\npml_write_files = function(x){\n    n = length(x)\n    for(i in 1:n){\n        filename = paste0(\"problem_id_\",i,\".txt\")\n        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\n    }\n}\n\npml_write_files(answers)\n",
    "created" : 1433470242900.000,
    "dirty" : true,
    "encoding" : "",
    "folds" : "",
    "hash" : "823213490",
    "id" : "A4F3D187",
    "lastKnownWriteTime" : 393812576,
    "path" : null,
    "project_path" : null,
    "properties" : {
        "tempName" : "Untitled2"
    },
    "source_on_save" : false,
    "type" : "r_source"
}