if ("pml-training.csv" %in% list.files(".")) {
print("yay!")
}
if ("pml-training.csv" %in% list.files(".")) {
print("yay!")
} else { print("boo")}
if ("pml-training.csv" %in% list.files(".")) {
print("yay!")
} else { print("downloading")
download.file(
"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
destfile="./pml-training.csv")
}
f ("pml-training.csv" %in% list.files(".")) {
print("yay!")
}
if ("pml-testing.csv" %in% list.files(".")) {
print("yay!")
}
if ("pml-training.csv" %in% list.files(".")) {
print("yay!")
}
if ("pml-testing.csv" %in% list.files(".")) {
} else { print("downloading")
download.file(
"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
destfile="./pml-esting.csv")
}
training<-read.csv("./pml-training.csv", header=T)
testing<-read.csv("./pnl-testing.csv", header=T)
testing<-read.csv("./pml-testing.csv", header=T)
if ("pml-testing.csv" %in% list.files(".")) {
} else { print("downloading")
download.file(
"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
destfile="./pml-testing.csv")
}
testing<-read.csv("./pml-testing.csv", header=T)
summary(training)[1:20]
summary(training)[1:20,]
summary(training)[,1:20]
summary(training[,1:20])
help(rowsum)
summary(training[,21:40])
attach(training)
head(stddev_roll_belt^2-var_roll_belt,20)
head(stddev_roll_belt)
summary(training[,41:60])
summary(training[,61:80])
summary(training[,81,100])
summary(training[,81:100])
summary(training[,101:120])
summary(training[,121:140])
summary(training[,141:160])
summary(training[,1:20])
dim(training[-c(1,3:7, 12:36, 50:59, 69:83, 87:101, 103:112,125:139, 141:150  )])
testing
1-(19216/19622)
testing[order(testing$cvtd_timestamp)]
testing[order(testing$cvtd_timestamp),]
trainingclean<-training[-c(1,3:7, 12:36, 50:59, 69:83, 87:101, 103:112,125:139, 141:150  )]
testingclean<-testing[-c(1,3:7, 12:36, 50:59, 69:83, 87:101, 103:112,125:139, 141:150  )]
summary(trainingclean)
d<-(sqrt(accel_belt_x^2+accel_belt_y^2+accel_belt_x^2)-total_accel_belt)
head(d)
d2<-sqrt(roll_belt^2+pitch_belt^2+yaw_belt^2)-total_accel_belt)
d2<-(sqrt(roll_belt^2+pitch_belt^2+yaw_belt^2)-total_accel_belt)
head(d2)
d3<-(sqrt(gyros_belt_x^2+gyros_belt_y^2+gyros_belt_x^2)-total_accel_belt)
head(d3)
head(total_accel_belt)
dim(trainingClean)[1]
dim(trainingclean)[1]
traininclean[,54]
trainingclean[,54]
outcomes<-c()
for (i in 1:dim(trainingclean)[1]) {
if (trainingclean[i,54]=="A"){
outcomes[i]<-"Right"
} else {
outcomes[i]<-"Wrong"
}
}
head(trainingclean[,54])
help(train)
library(caret)
help(train)
t<-train(trainingclean[,1:53], outcomes, method="rf")
t<-train(trainingclean[,1:53], outcomes, method="glm")
warnings()
summary(trainingclean)
t<-train(trainingclean[,2:53], outcomes, method="glm")
dettach(training)
detach(training)
t<-train(trainingclean[,2:53], outcomes, method="glm")
warnings()
t<-train(trainingclean[,2:5], outcomes, method="glm")
summary(trainingclean[,2:53])
str(trainingclean[,2:53])
trainingclean[,4]<-as.numeric(trainingclean[,4])
str(trainingclean[,2:53])
trainingclean[,5]<-as.numeric(trainingclean[,5])
str(trainingclean[,2:53])
t<-train(trainingclean[,2:5], outcomes, method="glm")
t<-train(outcomes~trainingclean[,2:53], method="glm")
t<-train(outcomes~., data=trainingclean[,2:53], method="glm")
t<-train(outcomes~., data=trainingclean[,2:53], method="lm")
t<-train(trainingclean[,2:5], outcomes, method="glm", na.action=na.omit)
library(mlbench)
data(Sonar)
set.seed(107)
inTrain <- createDataPartition(y = Sonar$Class,
+ ## the outcome data are needed
+ p = .75,
+ ## The percentage of data in the
+ ## training set
+ list = FALSE)
## The format of the results
## The output is a set of integers for the rows of Sonar
## that belong in the training set.
str(inTrain)
install.packages("mlbench")
library(mlbench)
data(Sonar)
set.seed(107)
inTrain <- createDataPartition(y = Sonar$Class,
+ ## the outcome data are needed
+ p = .75,
inTrain <- createDataPartition(y = Sonar$Class,
## the outcome data are needed
p = .75,
## The percentage of data in the
## training set
list = FALSE)
str(inTrain)
training <- Sonar[ inTrain,]
testing <- Sonar[-inTrain,]
nrow(training)
plsFit <- train(Class ~ ., data = training,  method = "pls",
## Center and scale the predictors for the training
## set and all future samples.
preProc = c("center", "scale"))
plsFit <- train(Class ~ ., data = training,  method = "pls",
## Center and scale the predictors for the training
## set and all future samples.
preProc = c("center", "scale"))
sum(complete.cases(trainingclean))
dim(trainingclean)
?glm
glm.1<-glm(outcomes~., data=trainingclean[,2:53])
glm.1<-glm(outcomes~., data=trainingclean[,2:53], family="logit")
glm.1<-glm(outcomes~., data=trainingclean[,2:53], family="binomial")
glm.1<-glm(outcomes~., data=trainingclean[,2:53], family="binomial(link=logit)")
glm.1<-glm(outcomes~., data=trainingclean[,2:53], family=("binomial", link="logit"))
glm.1<-glm(outcomes~., data=trainingclean[,2:53], family("binomial", link="logit"))
glm.1<-glm(outcomes~., data=trainingclean[,2:53], family(binomial, link="logit"))
glm.1<-glm(outcomes~., data=trainingclean[,2:53], family="binomial", link="logit")
glm.1<-glm(outcomes~., data=trainingclean[,2:53], family="quasibinomial")
head(outcomes)
str(outcomes)
?factor
outcomes<-factor(levels="Right", "Wrong")
for (i in 1:dim(trainingclean)[1]) {
if (trainingclean[i,54]=="A"){
outcomes[i]<-"Right"
} else {
outcomes[i]<-"Wrong"
}
}
str(outcomes)
head(outcomes)
outcomes<-factor(levels=c("Right", "Wrong"))
for (i in 1:dim(trainingclean)[1]) {
if (trainingclean[i,54]=="A"){
outcomes[i]<-"Right"
} else {
outcomes[i]<-"Wrong"
}
}
str(outcomes)
head(outcomes)
dim(outcomes)
length(outcomes)
t<-train(trainingclean[,2:53], outcomes, method="glm", na.action=na.omit)
library(caret)
t<-train(trainingclean[,2:53], outcomes, method="glm", na.action=na.omit)
warnings()
str(t)
t$results
rf.model<-train(trainingclean[,2:53], outcomes, method="rf")
nsv<- nearZeroVar(trainingclean, saveMetrics=TRUE)
nsv
M<-abs(cor(trainingclean[,2:53]))
diag(M)<-0
which(M>.8, arr.ind=T)
preProc<-preProcess(log10(trainingclean[,2:53]+.0001), method="pca", pcaComp=2)
preProc<-preProcess(log10(trainingclean[,2:53]+1), method="pca", pcaComp=2)
confusionMatrix(outcomes, predict(glm.model, trainingclean[,2:53]))
confusionMatrix(outcomes, predict(t, trainingclean[,2:53]))
training<-read.csv("./pml-training.csv", header=T)
testing<-read.csv("./pml-testing.csv", header=T)
library(caret)
trainIndex<-createDataPartition(training[,160], p=.8, list=FALSE)
validationclean<-training[-trainIndex,-c(1,2,3:7, 12:36, 50:59, 69:83, 87:101, 103:112,125:139, 141:150  )]
trainingclean<-training[trainIndex, -c(1,2,3:7, 12:36, 50:59, 69:83, 87:101, 103:112,125:139, 141:150  )]
set.seed(3450)
trainIndex<-createDataPartition(training[,160], p=.8, list=FALSE)
validationclean<-training[-trainIndex,-c(1,2,3:7, 12:36, 50:59, 69:83, 87:101, 103:112,125:139, 141:150  )]
trainingclean<-training[trainIndex, -c(1,2,3:7, 12:36, 50:59, 69:83, 87:101, 103:112,125:139, 141:150  )]
testingclean<-testing[-c(1,2,3:7, 12:36, 50:59, 69:83, 87:101, 103:112,125:139, 141:150  )]
outcomes<-factor(levels=c("Right", "Wrong"))
for (i in 1:dim(trainingclean)[1]) {
if (trainingclean[i,53]=="A"){
outcomes[i]<-"Right"
} else {
outcomes[i]<-"Wrong"
}
}
nsv<- nearZeroVar(trainingclean, saveMetrics=TRUE)
nsv
M<-abs(cor(trainingclean[,1:52]))
diag(M)<-0
which(M>.8, arr.ind=T)
library(caret)
glm.model<-train(trainingclean[,1:52], outcomes, method="glm", na.action=na.omit)
glm.model$results     #Accuracy of .902, not bad
confusionMatrix(outcomes, predict(glm.model, trainingclean[,1:52]))
rf.model<-train(trainingclean[,1:52], outcomes, method="rf")
rf.model<-train(trainingclean[,1:52], outcomes, method="rf")
preProc<-preProcess(trainingclean[,1:52], method="pca", pcaComp=2)
pcatrain<- predict(preProc, trainingclean[,1:52])
modelFit<- train(outcomes~., data=pcatrain, method="glm")
pcatest<- predict(preProc,testingclean[,1:52])
confusionMatrix(outcomes, predict(modelFit, testPC))
confusionMatrix(outcomes, predict(modelFit, pcatest))
head(testingclean)
validationoutcomes<-factor(levels=c("Right", "Wrong"))
for (i in 1:dim(validationclean)[1]) {
if (validationclean[i,53]=="A"){
validationoutcomes[i]<-"Right"
} else {
validationoutcomes[i]<-"Wrong"
}
}
confusionMatrix(validationoutcomes, predict(modelFit, pcatest))
pcavalidation<- predict(preProc,validationclean[,1:52])
confusionMatrix(validationoutcomes, predict(modelFit, pcavalidation))
modelFitt$result
modelFit$result
preProc<-preProcess(trainingclean[,1:52], method="pca", thresh=.8)
pcatrain<- predict(preProc, trainingclean[,1:52])
modelFit<- train(outcomes~., data=pcatrain, method="glm")
modelFit$result
pcavalidation<- predict(preProc,validationclean[,1:52])
confusionMatrix(validationoutcomes, predict(modelFit, pcavalidation))
confusionMatrix(validationoutcomes, predict(glm.model, validationclean[,1:52]))
preProc<-preProcess(trainingclean[,1:52], method="pca", thresh=.9)
pcatrain<- predict(preProc, trainingclean[,1:52])
modelFit<- train(outcomes~., data=pcatrain, method="glm")
modelFit$result
glm.model2<-train(trainingclean[,1:52], trainingclean[,53], method="glm")
warnings()
?train
names(getModelInfo())
rpart.model<-train(trainingclean[,1:52], trainingclean[,53], method="rpart")
rpart.model$result
confusionMatrix(validationoutcomes, predict(rpart.model, validationclean[,1:52]))
confusionMatrix(validationclean[,53], predict(rpart.model, validationclean[,1:52]))
rf.model<-train(trainingclean[,1:52], outcomes, method="rf")
rf.pca.model<-train(pcatrain, trainingclean[,53], method="rf")
